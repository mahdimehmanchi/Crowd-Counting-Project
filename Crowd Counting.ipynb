{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWfpV4YBVwrn"
      },
      "source": [
        "# Loading necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlObzIIny3pk"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "import random\n",
        "import h5py\n",
        "import scipy.io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "import torch\n",
        "import pickle\n",
        "import imgaug.augmenters as iaa \n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVzYjvQFWCZp"
      },
      "source": [
        "# Mounting Drive to the Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNbCvaRsy3p5",
        "outputId": "64c294bd-d071-44e5-fc73-57eef4d9ad08"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBfBk1URJdEa"
      },
      "source": [
        "# Preparing Train data & Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN0I2EJ1KIep"
      },
      "source": [
        "## Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nAHgfz9y3p8"
      },
      "outputs": [],
      "source": [
        "root_img = r\"/content/drive/My Drive/images\"\n",
        "root_label = r\"/content/drive/My Drive/ground-truth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD_nNEhQy3qA"
      },
      "outputs": [],
      "source": [
        "images_folder = root_img\n",
        "images_names = [f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqLYEjyGy3qC"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "matrix = 260\n",
        "sample = 9\n",
        "sample_list = []\n",
        "label_dict = []\n",
        "k = 0\n",
        "\n",
        "for image_name in images_names:\n",
        "    img = Image.open(root_img + \"/\" + image_name)\n",
        "    mat = scipy.io.loadmat(root_label + \"/\" + 'GT_' + image_name[:-4])\n",
        "    coordinates = mat['image_info'][0,0][0,0][0]\n",
        "    x, y = img.size\n",
        "    for i in range(sample):\n",
        "        x1 = randrange(0, x - matrix)\n",
        "        y1 = randrange(0, y - matrix)\n",
        "        sample_list.append(img.crop((x1, y1, x1 + matrix, y1 + matrix)))\n",
        "        label_list = []\n",
        "        for j in range(coordinates.shape[0]):\n",
        "            x_label = coordinates[j,0]\n",
        "            y_label = coordinates[j,1]\n",
        "            if x_label >= x1 and x_label <= x1+matrix and y_label >= y1 and y_label <= y1 + matrix:\n",
        "                x_new = (x_label - x1)/4\n",
        "                y_new = (y_label - y1)/4\n",
        "                label_list.append([int(x_new), int(y_new)])\n",
        "        label_dict.append(label_list)           \n",
        "        k += 1            \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t69Y2qf4y3qH"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "\n",
        "dens_list = []\n",
        "for i in range (len(label_dict)):\n",
        "    dens_map = torch.zeros((65,65))\n",
        "    for j in label_dict[i]:\n",
        "        dens_map[j[1], j[0]] = 1\n",
        "    dens_list.append(torch.from_numpy(gaussian_filter(dens_map, sigma = 3, mode = 'constant')))    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF9bVQyFK_EI"
      },
      "source": [
        "### Saving Train data in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpAm7Eqj42O2"
      },
      "outputs": [],
      "source": [
        "# save data\n",
        "with open(\"dens_train.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(dens_list, fp)    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz_Luq5tABWZ"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/dens_train.txt\" \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iToqXGkYCMgu"
      },
      "outputs": [],
      "source": [
        "# save data\n",
        "with open(\"img_train.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(sample_list, fp)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os2RyKFxCXvB"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/img_train.txt\" \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihRGThbQLEk4"
      },
      "source": [
        "### Showing a sample of Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "jnafOvnMtax4",
        "outputId": "39fa910b-0530-4828-a1c7-8f22add03a6a"
      },
      "outputs": [],
      "source": [
        "plt.imshow(sample_list[2])\n",
        "plt.show()\n",
        "plt.imshow(dens_list[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6dw9L--KPvL"
      },
      "source": [
        "## Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0fDreeJe0q1"
      },
      "outputs": [],
      "source": [
        "root_img = r\"/content/drive/My Drive/images-test\"\n",
        "root_label = r\"/content/drive/My Drive/ground-truth-test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3eHpDBnC06M"
      },
      "outputs": [],
      "source": [
        "images_folder = root_img\n",
        "images_names = [f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rM2p31fC28Z"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "matrix = 260\n",
        "sample = 9\n",
        "sample_list_test = []\n",
        "label_dict_test = []\n",
        "k = 0\n",
        "\n",
        "for image_name in images_names:\n",
        "    img = Image.open(root_img + \"/\" + image_name)\n",
        "    mat = scipy.io.loadmat(root_label + \"/\" + 'GT_' + image_name[:-4])\n",
        "    coordinates = mat['image_info'][0,0][0,0][0]\n",
        "    x, y = img.size\n",
        "    for i in range(sample):\n",
        "        x1 = randrange(0, x - matrix)\n",
        "        y1 = randrange(0, y - matrix)\n",
        "        sample_list_test.append(img.crop((x1, y1, x1 + matrix, y1 + matrix)))\n",
        "        label_list = []\n",
        "        for j in range(coordinates.shape[0]):\n",
        "            x_label = coordinates[j,0]\n",
        "            y_label = coordinates[j,1]\n",
        "            if x_label >= x1 and x_label <= x1+matrix and y_label >= y1 and y_label <= y1 + matrix:\n",
        "                x_new = (x_label - x1)/4\n",
        "                y_new = (y_label - y1)/4\n",
        "                label_list.append([int(x_new), int(y_new)])\n",
        "        label_dict_test.append(label_list)           \n",
        "        k += 1            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBygDnABDGJs"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "\n",
        "dens_list_test = []\n",
        "for i in range (len(label_dict_test)):\n",
        "    dens_map = torch.zeros((65,65))\n",
        "    for j in label_dict_test[i]:\n",
        "        dens_map[j[1], j[0]] = 1\n",
        "    dens_list_test.append(torch.from_numpy(gaussian_filter(dens_map, sigma = 3, mode = 'constant'))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1_kphR1LL7d"
      },
      "source": [
        "### Saving Test data in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xInWBqVDXbX"
      },
      "outputs": [],
      "source": [
        "# save data\n",
        "with open(\"dens_test.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(dens_list_test, fp) \n",
        "\n",
        "!cp \"/content/dens_test.txt\" \"/content/drive/My Drive\"       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeB4m5tvDdvb"
      },
      "outputs": [],
      "source": [
        "# save data\n",
        "with open(\"img_test.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(sample_list_test, fp)   \n",
        "\n",
        "!cp \"/content/img_test.txt\" \"/content/drive/My Drive\"    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBWE-tXyKXRl"
      },
      "source": [
        "# Loading Train data & Test data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQjqXJAuKWmj"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "with open(r\"/content/drive/My Drive/dens_train.txt\", \"rb\") as fp:\n",
        "  dens_list = pickle.load(fp)\n",
        "\n",
        "with open(r\"/content/drive/My Drive/img_train.txt\", \"rb\") as fp:\n",
        "  sample_list = pickle.load(fp)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KRz4YqOKWpL"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "with open(r\"/content/drive/My Drive/dens_test.txt\", \"rb\") as fp:\n",
        "  dens_list_test = pickle.load(fp)\n",
        "\n",
        "with open(r\"/content/drive/My Drive/img_test.txt\", \"rb\") as fp:\n",
        "  sample_list_test = pickle.load(fp)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEbixpisJNsS"
      },
      "outputs": [],
      "source": [
        "sample_list += sample_list_test\n",
        "dens_list += dens_list_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3owphYDPMTuq"
      },
      "source": [
        "# Dataloader class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-1gLR2Ot6gs"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, list_IDs,img_transform, aug=False):\n",
        "        self.list_IDs = list_IDs\n",
        "        self.img_transform = img_transform\n",
        "        self.aug = aug\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, ind):\n",
        "\n",
        "        # Load data and get label\n",
        "        X = self.load_img(ind)\n",
        "        y = self.load_label(ind)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "  def load_img(self, ind):\n",
        "    if self.aug == False:\n",
        "        img = sample_list[ind]\n",
        "    else: \n",
        "        img = new_images[ind]\n",
        "\n",
        "    transformed_img = self.img_transform(img) \n",
        "    return transformed_img\n",
        "\n",
        "  def load_label(self, ind):\n",
        "    if self.aug == False:\n",
        "        dens_map = dens_list[ind]\n",
        "    else:\n",
        "        dens_map = new_dens[ind]\n",
        "\n",
        "    return dens_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KnVSZxqMoC_"
      },
      "source": [
        "# Convolutional Neural Network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MrcnydBy3qR"
      },
      "outputs": [],
      "source": [
        "class CCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CCNN, self).__init__()\n",
        "      # convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, 11, padding = 5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 7, padding = 3)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 5, padding = 2)\n",
        "        self.conv4 = nn.Conv2d(64, 1000, 1, padding = 0)\n",
        "        self.conv5 = nn.Conv2d(1000, 400, 1, padding = 0)\n",
        "        self.conv6 = nn.Conv2d(400, 1, 1, padding = 0)\n",
        "        \n",
        "      # max pooling layers in encoder\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "      \n",
        "\n",
        "    def forward(self,x):\n",
        "       # First layer \n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "       # Second layer\n",
        "        x = self.pool1(F.relu(self.conv2(x)))\n",
        " \n",
        "       # Third layer\n",
        "        x = self.pool2(F.relu(self.conv3(x)))\n",
        "\n",
        "       # Fourth layer\n",
        "        x = F.relu(self.conv4(x))\n",
        "\n",
        "       # Fifth layer\n",
        "        x = F.relu(self.conv5(x))\n",
        "\n",
        "       # Sixth layer\n",
        "        x = F.relu(self.conv6(x))\n",
        "\n",
        "    \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoRYXntvPtlh"
      },
      "source": [
        "# Part 1: Training without data augmention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW9OcAirNB9x"
      },
      "source": [
        "## Defining Train loader, Validation loader, and Test loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyvoTjZUy3qN"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "\n",
        "train_indices = [] \n",
        "valid_indices = random.sample(range(0, 3600), 360)\n",
        "for i in range(len(dens_list)):\n",
        "    if i not in valid_indices:\n",
        "        train_indices.append(i)\n",
        "\n",
        "test_indices = [len(dens_list), len(dens_list)+len(dens_list_test)]        \n",
        "\n",
        "partition = {'train' : train_indices , 'validation' : valid_indices, 'test' : test_indices}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24T-o8_z3npu"
      },
      "outputs": [],
      "source": [
        "training_set = Dataset(partition['train'], transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=30,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "\n",
        "validation_set = Dataset(partition['validation'], transform)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=30,\n",
        "                                               shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fitc6fdgNavh"
      },
      "source": [
        "## Training the network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKAXtq6tNfU2"
      },
      "source": [
        "### Specifying training options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ryg6r4vGYxO"
      },
      "outputs": [],
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWGP9JtHy3qV"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "# Model \n",
        "Net = CCNN()\n",
        "Net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLngqBN9y3qW"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "def lossfunc(output, target):\n",
        "    return (torch.dist(output, target,p=2) /(2*output.shape[0]))**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_marhaUPy3qX"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "optimizer = optim.Adam(Net.parameters(), lr = 0.00005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d39jqzhty3qY"
      },
      "outputs": [],
      "source": [
        "max_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHY1zEvZy3qY"
      },
      "outputs": [],
      "source": [
        "transform = transforms.ToTensor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2uo_lUC_LwA"
      },
      "outputs": [],
      "source": [
        "mse = nn.MSELoss()\n",
        "mae = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJEJfvrgNrse"
      },
      "source": [
        "### Training main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX60-4dQy3qZ",
        "outputId": "1137c094-eeca-4f4d-e33f-d422f7790b94"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "train_losses = []\n",
        "validation_losses = []\n",
        "mse_train_losses = []\n",
        "mse_valid_losses = []\n",
        "mae_train_losses = []\n",
        "mae_valid_losses = []\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    batch_loss_train = 0\n",
        "    batch_loss_valid = 0\n",
        "\n",
        "    train_mse = 0\n",
        "    valid_mse = 0\n",
        "    batch_mse_train = 0\n",
        "    batch_mse_valid = 0\n",
        "\n",
        "    train_mae = 0\n",
        "    valid_mae = 0\n",
        "    batch_mae_train = 0\n",
        "    batch_mae_valid = 0\n",
        "\n",
        "    train_counter = 0\n",
        "    val_counter = 0 \n",
        "    \n",
        "    # Training\n",
        "    for batch_data, batch_labels in training_loader:\n",
        "\n",
        "        # Transfer to GPU\n",
        "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Model computations\n",
        "        batch_outputs = Net(batch_data)        \n",
        "        loss = lossfunc(batch_outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        label = torch.round(torch.sum(batch_labels, dim = [1,2]))\n",
        "        output = torch.round(torch.sum(batch_outputs.squeeze(), dim = [1,2]))\n",
        "\n",
        "        train_mse += torch.sqrt(mse(label,output))\n",
        "        train_mae += torch.sqrt(mae(label,output))\n",
        "\n",
        "        # print statistics\n",
        "        \n",
        "\n",
        "        print('Training: ', 'Epoch No: ', epoch+1, 'Iteration No: ', train_counter+1, '\\n',\n",
        "              'Loss: ', train_loss)\n",
        "        batch_loss_train += train_loss\n",
        "        batch_mse_train += train_mse\n",
        "        batch_mae_train += train_mae  \n",
        "        train_loss = 0\n",
        "        train_mse = 0\n",
        "        train_mae = 0      \n",
        "        train_counter += 1\n",
        "        \n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for batch_data, batch_labels in validation_loader:\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            \n",
        "            # Model computations\n",
        "            batch_outputs = Net(batch_data)            \n",
        "            loss = lossfunc(batch_outputs, batch_labels)\n",
        "            valid_loss += loss.item()\n",
        "             \n",
        "\n",
        "            label = torch.round(torch.sum(batch_labels, dim = [1,2]))\n",
        "            output = torch.round(torch.sum(batch_outputs.squeeze(), dim = [1,2]))\n",
        "\n",
        "            valid_mse += torch.sqrt(mse(label,output))\n",
        "            valid_mae += torch.sqrt(mae(label,output))\n",
        "\n",
        "            print('Validation: ', 'Epoch No: ', epoch+1, 'Iteration No: ', val_counter+1, '\\n',\n",
        "                  'Loss: ', valid_loss)\n",
        "            batch_loss_valid +=  valid_loss\n",
        "            batch_mse_valid += valid_mse\n",
        "            batch_mae_valid += valid_mae\n",
        "            valid_loss = 0\n",
        "            valid_mse = 0\n",
        "            valid_mae = 0   \n",
        "            val_counter += 1\n",
        "    \n",
        "    train_losses.append(batch_loss_train / train_counter)\n",
        "    validation_losses.append(batch_loss_valid / val_counter)\n",
        "    mse_train_losses.append(batch_mse_train / train_counter)\n",
        "    mse_valid_losses.append(batch_mse_valid / val_counter)\n",
        "    mae_train_losses.append(batch_mae_train / train_counter)\n",
        "    mae_valid_losses.append(batch_mae_valid / val_counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg-fxY52Nwy8"
      },
      "source": [
        "### Saving trained network \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XBC8_cUJF_y"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/My Drive/CCNN.pth'\n",
        "torch.save(Net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS0KA6jvN4dh"
      },
      "source": [
        "### Showing Train & Validation Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cA1W-tdQy3qa",
        "outputId": "839f432c-38ed-4429-e5d4-fa6ce1e89495"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses)\n",
        "plt.plot(validation_losses)\n",
        "plt.xlabel('Epoch No')\n",
        "plt.ylabel('Averaged loss')\n",
        "plt.title(\"CCNN loss function\")\n",
        "plt.legend(labels = [\"train\",\"validation\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "NNSjrefnHpth",
        "outputId": "f993b755-93db-49f8-a126-b181ed9350eb"
      },
      "outputs": [],
      "source": [
        "plt.plot(mse_train_losses)\n",
        "plt.plot(mse_valid_losses)\n",
        "plt.xlabel('Epoch No')\n",
        "plt.ylabel('Averaged MSE loss')\n",
        "plt.title(\"CCNN MSE loss\")\n",
        "plt.legend(labels = [\"train\",\"validation\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "qwrjsMiaJa4A",
        "outputId": "bf520918-9820-452b-ffe6-3ef460b63d6a"
      },
      "outputs": [],
      "source": [
        "plt.plot(mae_train_losses)\n",
        "plt.plot(mae_valid_losses)\n",
        "plt.xlabel('Epoch No')\n",
        "plt.ylabel('Averaged MAE loss')\n",
        "plt.title(\"CCNN MAE loss\")\n",
        "plt.legend(labels = [\"train\",\"validation\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r73euQYXTwRE"
      },
      "source": [
        "# Part 2: Training with data augmention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6J3U417VPc2"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yzVolYEV61-"
      },
      "outputs": [],
      "source": [
        "seq = iaa.Sequential([iaa.GammaContrast((0.5, 2.0)),\n",
        "                                        iaa.Affine(scale={\"x\": (0.5, 1.5), \"y\": (0.5, 1)}),\n",
        "                                        iaa.Affine(translate_px={\"x\": (-10, 10), \"y\": (-20, 10)}),\n",
        "                                        iaa.Affine(rotate=(-45, 45))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3I0J2ttVUJ_"
      },
      "outputs": [],
      "source": [
        "aug_sample = []\n",
        "aug_dens = []\n",
        "l = len(sample_list)\n",
        "aug_indices = random.sample(range(0, l), int(0.2*l))\n",
        "for i in range(l):\n",
        "  if i in aug_indices:\n",
        "    image_aug = seq(images=np.array(sample_list[i]))\n",
        "    dens_map_aug = seq(images = dens_list[i].numpy())\n",
        "    aug_sample.append(Image.fromarray(image_aug))\n",
        "    aug_dens.append(torch.from_numpy(dens_map_aug))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHHmqaUlUBVR"
      },
      "source": [
        "### Saving data in Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t1i-LzsYR3a"
      },
      "outputs": [],
      "source": [
        "# save data\n",
        "with open(\"dens_train_aug.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(aug_dens, fp)    \n",
        "\n",
        "with open(\"sample_train_aug.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(aug_sample, fp)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq4GjDJpYZrj"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/dens_train_aug.txt\" \"/content/drive/My Drive\"\n",
        "!cp \"/content/sample_train_aug.txt\" \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uvze4u1UMrI"
      },
      "source": [
        "### Loading data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9feEKtt9ZZso"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "with open(r\"/content/drive/My Drive/dens_train_aug.txt\", \"rb\") as fp:\n",
        "  dens_aug_list = pickle.load(fp)\n",
        "\n",
        "with open(r\"/content/drive/My Drive/sample_train_aug.txt\", \"rb\") as fp:\n",
        "  sample_aug_list = pickle.load(fp)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DSZySYyUhOW"
      },
      "source": [
        "### Defining Train loader, Validation loader, and Test loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foj3AcNlauf1"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "\n",
        "\n",
        "new_images = sample_list + sample_aug_list\n",
        "new_dens = dens_list + dens_aug_list\n",
        "\n",
        "train_indices_aug = [] \n",
        "valid_indices_aug = random.sample(range(0, len(new_images)), int(0.1*len(new_images)))\n",
        "for i in range(len(new_images)):\n",
        "    if i not in valid_indices_aug:\n",
        "        train_indices_aug.append(i)\n",
        "\n",
        "new_partition = {'train' : train_indices_aug , 'validation' : valid_indices_aug}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7tU0TRBdJ1Z"
      },
      "outputs": [],
      "source": [
        "training_set = Dataset(new_partition['train'], transform, aug=True)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(training_set, batch_size=30,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "\n",
        "validation_set = Dataset(new_partition['validation'], transform, aug=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=30,\n",
        "                                               shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv5iZbOsUmTy"
      },
      "source": [
        "### Specifying training options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8bkVvgGaukU"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "# Model \n",
        "new_Net = CCNN()\n",
        "new_Net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiBh-8vDaupM"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(new_Net.parameters(), lr = 0.00005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WriMJ-sddJyU"
      },
      "outputs": [],
      "source": [
        "max_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZemfPBC3dJ3z"
      },
      "outputs": [],
      "source": [
        "mse = nn.MSELoss()\n",
        "mae = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35hoqel_U2Kb"
      },
      "source": [
        "### Training main loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY1i2zXhdbVO",
        "outputId": "9205e96d-3765-45de-bbf1-9dff5aa4d08b"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "train_losses = []\n",
        "validation_losses = []\n",
        "mse_train_losses = []\n",
        "mse_valid_losses = []\n",
        "mae_train_losses = []\n",
        "mae_valid_losses = []\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(max_epochs):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    batch_loss_train = 0\n",
        "    batch_loss_valid = 0\n",
        "\n",
        "    train_mse = 0\n",
        "    valid_mse = 0\n",
        "    batch_mse_train = 0\n",
        "    batch_mse_valid = 0\n",
        "\n",
        "    train_mae = 0\n",
        "    valid_mae = 0\n",
        "    batch_mae_train = 0\n",
        "    batch_mae_valid = 0\n",
        "\n",
        "    train_counter = 0\n",
        "    val_counter = 0 \n",
        "    \n",
        "    # Training\n",
        "    for batch_data, batch_labels in training_loader:\n",
        "\n",
        "        # Transfer to GPU\n",
        "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Model computations\n",
        "        batch_outputs = new_Net(batch_data)        \n",
        "        loss = lossfunc(batch_outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        label = torch.round(torch.sum(batch_labels, dim = [1,2]))\n",
        "        output = torch.round(torch.sum(batch_outputs.squeeze(), dim = [1,2]))\n",
        "\n",
        "        train_mse += torch.sqrt(mse(label,output))\n",
        "        train_mae += torch.sqrt(mae(label,output))\n",
        "\n",
        "        # print statistics\n",
        "        \n",
        "\n",
        "        print('Training: ', 'Epoch No: ', epoch+1, 'Iteration No: ', train_counter+1, '\\n',\n",
        "              'Loss: ', train_loss)\n",
        "        batch_loss_train += train_loss\n",
        "        batch_mse_train += train_mse\n",
        "        batch_mae_train += train_mae  \n",
        "        train_loss = 0\n",
        "        train_mse = 0\n",
        "        train_mae = 0      \n",
        "        train_counter += 1\n",
        "        \n",
        "    # Validation\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for batch_data, batch_labels in validation_loader:\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            \n",
        "            # Model computations\n",
        "            batch_outputs = new_Net(batch_data)            \n",
        "            loss = lossfunc(batch_outputs, batch_labels)\n",
        "            valid_loss += loss.item()\n",
        "             \n",
        "\n",
        "            label = torch.round(torch.sum(batch_labels, dim = [1,2]))\n",
        "            output = torch.round(torch.sum(batch_outputs.squeeze(), dim = [1,2]))\n",
        "\n",
        "            valid_mse += torch.sqrt(mse(label,output))\n",
        "            valid_mae += torch.sqrt(mae(label,output))\n",
        "\n",
        "            print('Validation: ', 'Epoch No: ', epoch+1, 'Iteration No: ', val_counter+1, '\\n',\n",
        "                  'Loss: ', valid_loss)\n",
        "            batch_loss_valid +=  valid_loss\n",
        "            batch_mse_valid += valid_mse\n",
        "            batch_mae_valid += valid_mae\n",
        "            valid_loss = 0\n",
        "            valid_mse = 0\n",
        "            valid_mae = 0   \n",
        "            val_counter += 1\n",
        "    \n",
        "    train_losses.append(batch_loss_train / train_counter)\n",
        "    validation_losses.append(batch_loss_valid / val_counter)\n",
        "    mse_train_losses.append(batch_mse_train / train_counter)\n",
        "    mse_valid_losses.append(batch_mse_valid / val_counter)\n",
        "    mae_train_losses.append(batch_mae_train / train_counter)\n",
        "    mae_valid_losses.append(batch_mae_valid / val_counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWq7M_LWVIcq"
      },
      "source": [
        "### Saving trained network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ9-cI0Zejwk"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/My Drive/CCNN_aug.pth'\n",
        "torch.save(new_Net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UxacejGVSW6"
      },
      "source": [
        "### Showing Train & Validation Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ub3cM0ThekQU",
        "outputId": "0a979dfe-450a-471d-efd5-e7a78992d515"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses)\n",
        "plt.plot(validation_losses)\n",
        "plt.xlabel('Epoch No')\n",
        "plt.ylabel('Averaged loss')\n",
        "plt.title(\"CCNN with augmented data loss function\")\n",
        "plt.legend(labels = [\"train\",\"validation\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Ulr72FMgekat",
        "outputId": "7be69cde-fd7a-4224-c81d-26c4e28c47ee"
      },
      "outputs": [],
      "source": [
        "plt.plot(mse_train_losses)\n",
        "plt.plot(mse_valid_losses)\n",
        "plt.xlabel('Epoch No')\n",
        "plt.ylabel('Averaged MSE loss')\n",
        "plt.title(\"CCNN with augmented data MSE loss\")\n",
        "plt.legend(labels = [\"train\",\"validation\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "gZTjhN7le0lg",
        "outputId": "367e1a1d-8056-41c0-e9f9-3ec956e4c208"
      },
      "outputs": [],
      "source": [
        "plt.plot(mae_train_losses)\n",
        "plt.plot(mae_valid_losses)\n",
        "plt.xlabel('Epoch No')\n",
        "plt.ylabel('Averaged MAE loss')\n",
        "plt.title(\"CCNN with augmented data MAE loss\")\n",
        "plt.legend(labels = [\"train\",\"validation\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taAjDtHw_436"
      },
      "source": [
        "# Evaluating the performance on Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxxKWXPpWgWD"
      },
      "source": [
        "## Trained network without data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2opzTzOzGfmj"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/My Drive/CCNN.pth' \n",
        "net_test1 = CCNN()\n",
        "net_test1.load_state_dict(torch.load(PATH))\n",
        "net_test1.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSHR47vKGsys"
      },
      "outputs": [],
      "source": [
        "test_set = Dataset(partition['test'], transform)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4,\n",
        "                                             shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMztzwptMVL2",
        "outputId": "de735de2-979b-448d-a411-ef1f669c1e97"
      },
      "outputs": [],
      "source": [
        "test_mae = 0\n",
        "test_mse = 0\n",
        "test_loss = 0\n",
        "counter = 0\n",
        "\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            counter += 1\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            \n",
        "            # Model computations\n",
        "            batch_outputs = net_test1(batch_data)            \n",
        "            loss = lossfunc(batch_outputs, batch_labels)\n",
        "            test_loss += loss.item()\n",
        "             \n",
        "\n",
        "            label = torch.round(torch.sum(batch_labels, dim = [1,2]))\n",
        "            output = torch.round(torch.sum(batch_outputs.squeeze(), dim = [1,2]))\n",
        "\n",
        "            if counter == 1:\n",
        "              sample_batch_data = batch_data\n",
        "              sample_batch_labels = batch_labels\n",
        "              sample_batch_outputs = batch_outputs\n",
        "\n",
        "\n",
        "            test_mse += torch.sqrt(mse(label,output))\n",
        "            test_mae += torch.sqrt(mae(label,output))\n",
        "\n",
        "print('Mean loss on test set', test_loss/len(partition['test']))        \n",
        "print('Mean MSE on test set', test_mse/len(partition['test']))\n",
        "print('Mean MAE on test set', test_mae/len(partition['test']))        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujCV_2enQbOA"
      },
      "outputs": [],
      "source": [
        "img1 = sample_batch_data[0]\n",
        "label1 = sample_batch_labels[0]\n",
        "out1 = sample_batch_outputs[0,0]\n",
        "\n",
        "img2 = sample_batch_data[1]\n",
        "label2 = sample_batch_labels[1]\n",
        "out2 = sample_batch_outputs[1,0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "nxxRZ8EWTH1K",
        "outputId": "59dc45df-fa96-4c02-ad26-25c7f523f75e"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img1.cpu().permute(1, 2, 0))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(label1.cpu())\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(out1.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "UXIr8BF0eX9b",
        "outputId": "08023660-204d-4e8b-ebfd-784def3a436b"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img2.cpu().permute(1, 2, 0))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(label2.cpu())\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(out2.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcwcMsp0XQ3n"
      },
      "source": [
        "## Trained network with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn12S44eU0Qh"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/My Drive/CCNN_aug.pth' \n",
        "net_test2 = CCNN()\n",
        "net_test2.load_state_dict(torch.load(PATH))\n",
        "net_test2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzWKwLEKXexs",
        "outputId": "8f10e62a-87a3-453a-c3be-06928a4d3cfa"
      },
      "outputs": [],
      "source": [
        "test_mae = 0\n",
        "test_mse = 0\n",
        "test_loss = 0\n",
        "counter = 0\n",
        "\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            counter += 1\n",
        "            # Transfer to GPU\n",
        "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
        "            \n",
        "            # Model computations\n",
        "            batch_outputs = net_test2(batch_data)            \n",
        "            loss = lossfunc(batch_outputs, batch_labels)\n",
        "            test_loss += loss.item()\n",
        "             \n",
        "\n",
        "            label = torch.round(torch.sum(batch_labels, dim = [1,2]))\n",
        "            output = torch.round(torch.sum(batch_outputs.squeeze(), dim = [1,2]))\n",
        "\n",
        "            if counter == 1:\n",
        "              sample_batch_data = batch_data\n",
        "              sample_batch_labels = batch_labels\n",
        "              sample_batch_outputs = batch_outputs\n",
        "\n",
        "\n",
        "            test_mse += torch.sqrt(mse(label,output))\n",
        "            test_mae += torch.sqrt(mae(label,output))\n",
        "\n",
        "print('Mean loss on test set', test_loss/len(partition['test']))        \n",
        "print('Mean MSE on test set', test_mse/len(partition['test']))\n",
        "print('Mean MAE on test set', test_mae/len(partition['test']))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdFDHEu_Xe0z"
      },
      "outputs": [],
      "source": [
        "img1 = sample_batch_data[0]\n",
        "label1 = sample_batch_labels[0]\n",
        "out1 = sample_batch_outputs[0,0]\n",
        "\n",
        "img2 = sample_batch_data[1]\n",
        "label2 = sample_batch_labels[1]\n",
        "out2 = sample_batch_outputs[1,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "nXHWo_-6XykK",
        "outputId": "7933eb7c-31fd-441b-a50a-e77b33e6d98b"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img1.cpu().permute(1, 2, 0))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(label1.cpu())\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(out1.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "WxjH8duJer7n",
        "outputId": "afbc6e50-d585-4cea-dcc0-6236663858e4"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img2.cpu().permute(1, 2, 0))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(label2.cpu())\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(out2.cpu())\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep Learning-HW2-Q2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
